#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¯¹è¯æœåŠ¡æµ‹è¯•è„šæœ¬

æµ‹è¯•ChatServiceçš„å„é¡¹åŠŸèƒ½
"""

import sys
import os
import logging

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.config_manager import ConfigManager
from src.chat_service import ChatService
from src.embedding_model import EmbeddingModel
from src.vector_store import VectorStore
from src.retriever import RAGRetriever
from src.document_processor import DocumentProcessor

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def test_chat_service():
    """æµ‹è¯•å¯¹è¯æœåŠ¡åŸºæœ¬åŠŸèƒ½"""
    print("=" * 50)
    print("æµ‹è¯•å¯¹è¯æœåŠ¡åŸºæœ¬åŠŸèƒ½")
    print("=" * 50)
    
    try:
        # 1. åˆå§‹åŒ–é…ç½®ç®¡ç†å™¨
        config_manager = ConfigManager()
        config_manager.load_config()
        
        # 2. åˆå§‹åŒ–å¯¹è¯æœåŠ¡ï¼ˆè·³è¿‡APIå®¢æˆ·ç«¯åˆå§‹åŒ–ï¼‰
        chat_service = ChatService.__new__(ChatService)
        chat_service.config_manager = config_manager
        chat_service.config = config_manager.get_config()
        chat_service.client = None  # æ¨¡æ‹Ÿæ¨¡å¼ï¼Œä¸åˆå§‹åŒ–çœŸå®å®¢æˆ·ç«¯
        
        # APIé…ç½®
        chat_service.model_name = chat_service.config.llm_model
        chat_service.max_tokens = chat_service.config.llm_max_tokens
        chat_service.temperature = chat_service.config.llm_temperature
        chat_service.max_retries = 3
        chat_service.retry_delay = 1
        
        print("âœ“ å¯¹è¯æœåŠ¡åˆå§‹åŒ–æˆåŠŸï¼ˆæ¨¡æ‹Ÿæ¨¡å¼ï¼‰")
        
        # 3. æµ‹è¯•é…ç½®åŠ è½½
        print(f"âœ“ æ¨¡å‹é…ç½®: {chat_service.model_name}")
        print(f"âœ“ æœ€å¤§tokens: {chat_service.max_tokens}")
        print(f"âœ“ æ¸©åº¦å‚æ•°: {chat_service.temperature}")
        
        return True
        
    except Exception as e:
        print(f"âœ— å¯¹è¯æœåŠ¡æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_rag_integration():
    """æµ‹è¯•RAGé›†æˆåŠŸèƒ½"""
    print("\n" + "=" * 50)
    print("æµ‹è¯•RAGé›†æˆåŠŸèƒ½")
    print("=" * 50)
    
    try:
        # 1. åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶
        config_manager = ConfigManager()
        config_manager.load_config()
        
        # åˆ›å»ºæ¨¡æ‹ŸChatServiceå®ä¾‹
        chat_service = ChatService.__new__(ChatService)
        chat_service.config_manager = config_manager
        chat_service.config = config_manager.get_config()
        chat_service.client = None
        chat_service.model_name = chat_service.config.llm_model
        chat_service.max_tokens = chat_service.config.llm_max_tokens
        chat_service.temperature = chat_service.config.llm_temperature
        chat_service.max_retries = 3
        chat_service.retry_delay = 1
        
        # æ·»åŠ æ¨¡æ‹Ÿæµå¼APIè°ƒç”¨æ–¹æ³•
        def mock_call_api_stream(prompt):
            response_text = "è¿™æ˜¯åŸºäºæä¾›çš„çŸ¥è¯†åº“å†…å®¹ç”Ÿæˆçš„æ¨¡æ‹Ÿå›ç­”ã€‚é—®é¢˜ç›¸å…³çš„å…³é”®ä¿¡æ¯å·²åœ¨ä¸Šä¸‹æ–‡ä¸­æ‰¾åˆ°ã€‚"
            for chunk in response_text.split():
                yield {"choices": [{"delta": {"content": chunk + " "}}]}
        
        chat_service.call_api_stream = mock_call_api_stream
        
        # 2. å‡†å¤‡æµ‹è¯•æ–‡æ¡£
        from src.document_processor import DocumentChunk
        test_documents = [
            DocumentChunk(
                id="test1",
                content="æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿåœ¨æ²¡æœ‰æ˜ç¡®ç¼–ç¨‹çš„æƒ…å†µä¸‹å­¦ä¹ å’Œæ”¹è¿›ã€‚",
                metadata={"source": "test1.txt"},
                source_file="test1.txt",
                chunk_index=0,
                start_pos=0,
                end_pos=50
            ),
            DocumentChunk(
                id="test2",
                content="æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é›†ï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘çš„å·¥ä½œæ–¹å¼ã€‚",
                metadata={"source": "test2.txt"},
                source_file="test2.txt",
                chunk_index=0,
                start_pos=0,
                end_pos=40
            ),
            DocumentChunk(
                id="test3",
                content="è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé¢†åŸŸï¼Œä¸“æ³¨äºè®¡ç®—æœºä¸äººç±»è¯­è¨€ä¹‹é—´çš„äº¤äº’ã€‚",
                metadata={"source": "test3.txt"},
                source_file="test3.txt",
                chunk_index=0,
                start_pos=0,
                end_pos=45
            ),
            DocumentChunk(
                id="test4",
                content="è®¡ç®—æœºè§†è§‰æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œæ—¨åœ¨è®©è®¡ç®—æœºèƒ½å¤Ÿç†è§£å’Œè§£é‡Šè§†è§‰ä¿¡æ¯ã€‚",
                metadata={"source": "test4.txt"},
                source_file="test4.txt",
                chunk_index=0,
                start_pos=0,
                end_pos=40
            )
        ]
        
        # åˆ›å»ºåµŒå…¥æ¨¡å‹
        embedding_model = EmbeddingModel(model_name="BAAI/bge-small-zh-v1.5", device="cpu")
        embedding_model.load_model()  # åŠ è½½æ¨¡å‹
        
        # 4. åˆå§‹åŒ–æ£€ç´¢å™¨
        retriever = RAGRetriever(embedding_model)
        
        # å¤„ç†æ–‡æ¡£å¹¶æ„å»ºç´¢å¼•
        chunks = test_documents
        
        retriever.vector_store.build_index(chunks)
        print("âœ“ å‘é‡ç´¢å¼•æ„å»ºå®Œæˆ")
        
        # 5. æµ‹è¯•é—®ç­”
        test_questions = [
            "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
            "æ·±åº¦å­¦ä¹ å’Œæœºå™¨å­¦ä¹ æœ‰ä»€ä¹ˆå…³ç³»ï¼Ÿ",
            "NLPæ˜¯ä»€ä¹ˆï¼Ÿ",
            "è®¡ç®—æœºè§†è§‰çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ"
        ]
        
        for question in test_questions:
            print(f"\né—®é¢˜: {question}")
            
            # ä½¿ç”¨æµå¼æ–¹æ³•è¿›è¡Œæµ‹è¯•
            sources = []
            confidence = 0.0
            response_time = 0.0
            answer_parts = []
            error_msg = None
            
            try:
                for chunk_data in chat_service.generate_answer_stream(question, retriever):
                    if chunk_data['type'] == 'start':
                        sources = chunk_data['sources']
                        confidence = chunk_data['confidence']
                    elif chunk_data['type'] == 'chunk':
                        answer_parts.append(chunk_data['content'])
                    elif chunk_data['type'] == 'end':
                        response_time = chunk_data['response_time']
                    elif chunk_data['type'] == 'error':
                        error_msg = chunk_data['error']
                        break
                
                answer = ''.join(answer_parts)
                print(f"å›ç­”: {answer[:100]}...")
                print(f"æ¥æº: {', '.join(sources)}")
                print(f"ç½®ä¿¡åº¦: {confidence:.3f}")
                print(f"å“åº”æ—¶é—´: {response_time:.2f}ç§’")
                
                if error_msg:
                    print(f"é”™è¯¯: {error_msg}")
                    
            except Exception as e:
                print(f"æµå¼æµ‹è¯•é”™è¯¯: {e}")
        
        print("\nâœ“ RAGé›†æˆæµ‹è¯•å®Œæˆ")
        return True
        
    except Exception as e:
        print(f"âœ— RAGé›†æˆæµ‹è¯•å¤±è´¥: {e}")
        return False

def test_prompt_building():
    """æµ‹è¯•æç¤ºè¯æ„å»º"""
    print("\n" + "=" * 50)
    print("æµ‹è¯•æç¤ºè¯æ„å»º")
    print("=" * 50)
    
    try:
        config_manager = ConfigManager()
        config_manager.load_config()
        
        # åˆ›å»ºæ¨¡æ‹ŸChatServiceå®ä¾‹
        chat_service = ChatService.__new__(ChatService)
        chat_service.config_manager = config_manager
        chat_service.config = config_manager.get_config()
        chat_service.client = None
        
        question = "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ"
        context = "äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œæ—¨åœ¨åˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚"
        
        prompt = chat_service.build_prompt(question, context)
        
        print("æ„å»ºçš„æç¤ºè¯:")
        print("-" * 30)
        print(prompt)
        print("-" * 30)
        
        # éªŒè¯æç¤ºè¯åŒ…å«å¿…è¦å…ƒç´ 
        assert question in prompt, "æç¤ºè¯åº”åŒ…å«ç”¨æˆ·é—®é¢˜"
        assert context in prompt, "æç¤ºè¯åº”åŒ…å«ä¸Šä¸‹æ–‡"
        assert "çŸ¥è¯†åº“å†…å®¹" in prompt, "æç¤ºè¯åº”åŒ…å«çŸ¥è¯†åº“æ ‡è¯†"
        
        print("âœ“ æç¤ºè¯æ„å»ºæµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"âœ— æç¤ºè¯æ„å»ºæµ‹è¯•å¤±è´¥: {e}")
        return False

def test_error_handling():
    """æµ‹è¯•é”™è¯¯å¤„ç†"""
    print("\n" + "=" * 50)
    print("æµ‹è¯•é”™è¯¯å¤„ç†")
    print("=" * 50)
    
    try:
        config_manager = ConfigManager()
        config_manager.load_config()
        
        # åˆ›å»ºæ¨¡æ‹ŸChatServiceå®ä¾‹
        chat_service = ChatService.__new__(ChatService)
        chat_service.config_manager = config_manager
        chat_service.config = config_manager.get_config()
        chat_service.client = None
        
        # åˆ›å»ºåµŒå…¥æ¨¡å‹
        embedding_model = EmbeddingModel(model_name="BAAI/bge-small-zh-v1.5", device="cpu")
        
        # æµ‹è¯•ä¼ å…¥None retrieverçš„æƒ…å†µï¼ˆåº”è¯¥å¼•å‘å¼‚å¸¸ï¼‰
        try:
            error_found = False
            for chunk_data in chat_service.generate_answer_stream("æµ‹è¯•é—®é¢˜", None):
                if chunk_data['type'] == 'error':
                    print(f"é”™è¯¯ä¿¡æ¯: {chunk_data['error']}")
                    print(f"é”™è¯¯å†…å®¹: {chunk_data.get('content', '')}")
                    error_found = True
                    break
            
            # éªŒè¯é”™è¯¯å¤„ç†
            assert error_found, "åº”è¯¥æœ‰é”™è¯¯ä¿¡æ¯"
            
        except Exception as e:
            # å¦‚æœç›´æ¥æŠ›å‡ºå¼‚å¸¸ä¹Ÿæ˜¯å¯ä»¥æ¥å—çš„
            print(f"æ•è·åˆ°å¼‚å¸¸: {e}")
            assert e is not None, "å¼‚å¸¸å¯¹è±¡ä¸åº”è¯¥ä¸ºNone"
        
        print("âœ“ é”™è¯¯å¤„ç†æµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"âœ— é”™è¯¯å¤„ç†æµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("å¼€å§‹ChatServiceåŠŸèƒ½æµ‹è¯•")
    print("=" * 60)
    
    test_results = []
    
    # è¿è¡Œå„é¡¹æµ‹è¯•
    test_results.append(("å¯¹è¯æœåŠ¡åŸºæœ¬åŠŸèƒ½", test_chat_service()))
    test_results.append(("æç¤ºè¯æ„å»º", test_prompt_building()))
    test_results.append(("é”™è¯¯å¤„ç†", test_error_handling()))
    test_results.append(("RAGé›†æˆåŠŸèƒ½", test_rag_integration()))
    
    # è¾“å‡ºæµ‹è¯•ç»“æœ
    print("\n" + "=" * 60)
    print("æµ‹è¯•ç»“æœæ±‡æ€»")
    print("=" * 60)
    
    passed = 0
    total = len(test_results)
    
    for test_name, result in test_results:
        status = "âœ“ é€šè¿‡" if result else "âœ— å¤±è´¥"
        print(f"{test_name}: {status}")
        if result:
            passed += 1
    
    print(f"\næ€»è®¡: {passed}/{total} é¡¹æµ‹è¯•é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ChatServiceåŠŸèƒ½æ­£å¸¸")
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®å’Œç½‘ç»œè¿æ¥")

if __name__ == "__main__":
    main()